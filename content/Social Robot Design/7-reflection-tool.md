---
date: "2025-06-09"
title: "Session 7: Reflection Tool"
ShowToc: true
TocOpen: true

---

### Description
The goal of this tool is to help users identify, understand, and reflect on ethical concerns in social robotics based on their own chosen scenario or idea, using an interactive set of ethical dilemma cards and reflection. This tool is designed to support designers, students, and other stakeholders in exploring the ethical landscape of social robotics. By inputting their own scenario, they can use the ethical cards to uncover relevant dilemmas and have guided discussions that aim to improve the design of social robots.

![Scenario](/img/week7-cards.png) 

### SMART
**Specific:** Use the tool to help users reflect on ethical concerns in social robotics using their own scenario as a starting point.

**Measurable:** The answers to the prompts will be written down to create an ethical reflection of their scenario. With this reflection users are able to come up with design adaptations for the social robot in their scenario.

**Achievable:** The cards are a simple entry point to open up discussions about the ethical side of social robotics. 

**Relevant:** The cards addresses the need for ethical awareness in the fast-growing field of social robotics.

**Time-bound:** There is no time-bound for using the cards and having the discussions. It is difficult to put a time-bound on a discussion from which you don’t know what is happening without being there. The observer writes down all the important parts for the ethical reflection and guides people into exploring certain ethical cards. 


### Using the Tool
Firstly, users describe their own robot scenario or concept and write this in the start of the reflection report. After that, a card is picked which presents relevant ethical dilemmas selected from categories like autonomy, privacy, human-robot interaction, and more.

Each card includes:
- Title
- Description of the ethical issue
- Discussion prompt

Users reflect on how the dilemma applies to their specific scenario and document their discussion points, decisions, or design adjustments.

### Evaluating the tool outcomes
The tool was tested using the example of a robot that helps a client get ready for bed, such as turning off the TV and closing the curtains. During this test, it became clear that having a template for the observer would be helpful. This template could make it easier to keep track of what happens in the scenario, what is discussed, and what decisions are made. The tool works well because it does not provide a fixed list of ethical issues. Instead, people can explore the themes that best fit their own situation, which allows for flexible decision-making based on each specific case. It also became clear that asking "why" at each step is very important, as it helps people think more carefully about their choices and the reasons behind them.

### POEM observation framework
Observations were recorded using the POEM framework.

![Scenario](/img/Poem-week7.png) 

### Evaluating outcomes regarding the case + literature: 
Using the cards gave helpful ideas about some ethical questions. The cards used were:
![Scenario](/img/week7-cards-used.png)  

**Robot autonomy**
Answer to the autonomy questions:
For the bedtime assistance scenario, the robot should have limited autonomy with clear boundaries.

It can autonomously perform simple, routine tasks (like turning off the TV, closing curtains, dimming lights) once the bedtime sequence is initiated, either by the elderly or by a pre-set schedule agreed upon in advance.

However, when it comes to actions that directly involve the client’s comfort or safety (for example, moving near the client, giving reminders, adjusting the bed), the robot should always wait for user confirmation or input.

Because bedtime is a personal, potentially vulnerable moment, maintaining the client’s sense of control is essential to avoid feelings of being "managed" by the robot or invaded in their private space. Too much robot autonomy could result in loss of trust or frustration.

Wilson et al. highlight that socially assistive robots must be designed to support user autonomy, especially when working with vulnerable people such as elderly [1].

**Antropomorphic**
Answer to the anthropomorphic questions:
In this case, the robot should not be highly humanlike.

A soft, friendly, but clearly mechanical design is appropriate. For example, it can have simple expressive features (like a face with eyes that can blink or change expression), but its overall body should remain visibly robotic.

Avoid detailed facial features and skin-like textures, which could trigger the uncanny valley effect.

The bedtime routine is intimate, and users may feel more at ease if the robot's appearance communicates a clear, supportive, but not human-replacing role.

Strait et al., demonstrate that highly humanlike robots can provoke strong aversion and that robot designers should carefully balance humanlike traits to avoid triggering discomfort and loss of trust [2].

### Sources:
[1] K. Yogeeswaran, "The interactive effects of robot anthropomorphism and robot ability on perceived threat and support for robotics research," Journal of Human-Robot Interaction, vol. 5, no. 2, pp. 29–47, 2016. [Online]. Available: https://doi.org/10.5898/JHRI.5.2.Yogeeswaran

[2] M. B. Mathur and D. B. Reichling, "Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley," Cognition, vol. 146, pp. 22–32, Jan. 2016. [Online]. Available: https://doi.org/10.1016/j.cognition.2015.09.008.


